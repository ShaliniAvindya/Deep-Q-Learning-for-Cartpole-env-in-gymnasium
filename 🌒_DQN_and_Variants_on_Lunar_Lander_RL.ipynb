{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30512,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "ðŸŒ’ DQN and Variants on Lunar Lander - RL",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAvindya/Deep-Q-Learning-for-Cartpole-env-in-gymnasium/blob/main/%F0%9F%8C%92_DQN_and_Variants_on_Lunar_Lander_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Learning and Variants in Gym's Lunar Lander Environment\n",
        "\n",
        "In this notebook, we will explore the implementation of a Deep Q-Learning (DQN) agent to navigate Gym's Lunar Lander environment.\n",
        "\n",
        "We will use apply four variants of the DQN algorithm:\n",
        "- The Classic DQN (Mihn et al 2013)\n",
        "- Double DQN (Hasselt et al 2015)\n",
        "- Dueling DQN (Wang et al 2015)\n",
        "- Double Dueling DQN\n",
        "\n",
        "In the Lunar Lander environment, the agent's task is to learn how to land a lunar module safely on the moon's surface. This requires the agent to balance fuel efficiency and safety considerations. The agent needs to learn from its past experiences, developing a strategy to approach the landing pad while minimizing its speed and using as little fuel as possible.\n",
        "\n",
        "All reinforcement learning (RL) methods will be built from scratch, providing a comprehensive understanding of their workings and we will use PyTorch to build our neural network model.\n",
        "\n",
        "Let's initialize a LunarLander-v2 environmnet, make random actions in the environment, then view a recording of it."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-06-21T19:20:26.339215Z",
          "iopub.execute_input": "2023-06-21T19:20:26.339632Z",
          "iopub.status.idle": "2023-06-21T19:20:26.345129Z",
          "shell.execute_reply.started": "2023-06-21T19:20:26.3396Z",
          "shell.execute_reply": "2023-06-21T19:20:26.343795Z"
        },
        "id": "ULHhZZU371xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-11T18:19:37.322029Z",
          "iopub.execute_input": "2024-05-11T18:19:37.322899Z",
          "iopub.status.idle": "2024-05-11T18:19:37.326949Z",
          "shell.execute_reply.started": "2024-05-11T18:19:37.322865Z",
          "shell.execute_reply": "2024-05-11T18:19:37.325951Z"
        },
        "trusted": true,
        "id": "7IMLG1QK71x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use in Kaggle we need to install these two packages\n",
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-11T18:19:37.657863Z",
          "iopub.execute_input": "2024-05-11T18:19:37.65817Z",
          "iopub.status.idle": "2024-05-11T18:20:45.939733Z",
          "shell.execute_reply.started": "2024-05-11T18:19:37.658146Z",
          "shell.execute_reply": "2024-05-11T18:20:45.9386Z"
        },
        "trusted": true,
        "id": "c7WLmVFq71yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from typing import TYPE_CHECKING, Optional\n",
        "import pickle\n",
        "import numpy as np\n",
        "# from DQNAgent import DQNAgent, DQN, ReplayBuffer\n",
        "import gym\n",
        "from gym import error, spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "from gym.utils import EzPickle, colorize\n",
        "from gym.utils.step_api_compatibility import step_api_compatibility\n",
        "\n",
        "try:\n",
        "    import Box2D\n",
        "    from Box2D.b2 import (\n",
        "        circleShape,\n",
        "        contactListener,\n",
        "        edgeShape,\n",
        "        fixtureDef,\n",
        "        polygonShape,\n",
        "        revoluteJointDef,\n",
        "    )\n",
        "except ImportError:\n",
        "    raise DependencyNotInstalled(\"box2d is not installed, run `pip install gym[box2d]`\")\n",
        "\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    import pygame\n",
        "\n",
        "\n",
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400\n",
        "\n",
        "\n",
        "class LunarLander(gym.Env, EzPickle):\n",
        "    \"\"\"\n",
        "    ### Description\n",
        "    This environment is a classic rocket trajectory optimization problem.\n",
        "    According to Pontryagin's maximum principle, it is optimal to fire the\n",
        "    engine at full throttle or turn it off. This is the reason why this\n",
        "    environment has discrete actions: engine on or off.\n",
        "\n",
        "    There are two environment versions: discrete or continuous.\n",
        "    The landing pad is always at coordinates (0,0). The coordinates are the\n",
        "    first two numbers in the state vector.\n",
        "    Landing outside of the landing pad is possible. Fuel is infinite, so an agent\n",
        "    can learn to fly and then land on its first attempt.\n",
        "\n",
        "    To see a heuristic landing, run:\n",
        "    ```\n",
        "    python gym/envs/box2d/lunar_lander.py\n",
        "    ```\n",
        "    <!-- To play yourself, run: -->\n",
        "    <!-- python examples/agents/keyboard_agent.py LunarLander-v2 -->\n",
        "\n",
        "    ### Action Space\n",
        "    There are four discrete actions available: do nothing, fire left\n",
        "    orientation engine, fire main engine, fire right orientation engine.\n",
        "\n",
        "    ### Observation Space\n",
        "    The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear\n",
        "    velocities in `x` & `y`, its angle, its angular velocity, and two booleans\n",
        "    that represent whether each leg is in contact with the ground or not.\n",
        "\n",
        "    ### Rewards\n",
        "    After every step a reward is granted. The total reward of an episode is the\n",
        "    sum of the rewards for all the steps within that episode.\n",
        "\n",
        "    For each step, the reward:\n",
        "    - is increased/decreased the closer/further the lander is to the landing pad.\n",
        "    - is increased/decreased the slower/faster the lander is moving.\n",
        "    - is decreased the more the lander is tilted (angle not horizontal).\n",
        "    - is increased by 10 points for each leg that is in contact with the ground.\n",
        "    - is decreased by 0.03 points each frame a side engine is firing.\n",
        "    - is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "    The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "    An episode is considered a solution if it scores at least 200 points.\n",
        "\n",
        "    ### Starting State\n",
        "    The lander starts at the top center of the viewport with a random initial\n",
        "    force applied to its center of mass.\n",
        "\n",
        "    ### Episode Termination\n",
        "    The episode finishes if:\n",
        "    1) the lander crashes (the lander body gets in contact with the moon);\n",
        "    2) the lander gets outside of the viewport (`x` coordinate is greater than 1);\n",
        "    3) the lander is not awake. From the [Box2D docs](https://box2d.org/documentation/md__d_1__git_hub_box2d_docs_dynamics.html#autotoc_md61),\n",
        "        a body which is not awake is a body which doesn't move and doesn't\n",
        "        collide with any other body:\n",
        "    > When Box2D determines that a body (or group of bodies) has come to rest,\n",
        "    > the body enters a sleep state which has very little CPU overhead. If a\n",
        "    > body is awake and collides with a sleeping body, then the sleeping body\n",
        "    > wakes up. Bodies will also wake up if a joint or contact attached to\n",
        "    > them is destroyed.\n",
        "\n",
        "    ### Arguments\n",
        "    To use to the _continuous_ environment, you need to specify the\n",
        "    `continuous=True` argument like below:\n",
        "    ```python\n",
        "    import gym\n",
        "    env = gym.make(\n",
        "        \"LunarLander-v2\",\n",
        "        continuous: bool = False,\n",
        "        gravity: float = -10.0,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        turbulence_power: float = 1.5,\n",
        "    )\n",
        "    ```\n",
        "    If `continuous=True` is passed, continuous actions (corresponding to the throttle of the engines) will be used and the\n",
        "    action space will be `Box(-1, +1, (2,), dtype=np.float32)`.\n",
        "    The first coordinate of an action determines the throttle of the main engine, while the second\n",
        "    coordinate specifies the throttle of the lateral boosters.\n",
        "    Given an action `np.array([main, lateral])`, the main engine will be turned off completely if\n",
        "    `main < 0` and the throttle scales affinely from 50% to 100% for `0 <= main <= 1` (in particular, the\n",
        "    main engine doesn't work  with less than 50% power).\n",
        "    Similarly, if `-0.5 < lateral < 0.5`, the lateral boosters will not fire at all. If `lateral < -0.5`, the left\n",
        "    booster will fire, and if `lateral > 0.5`, the right booster will fire. Again, the throttle scales affinely\n",
        "    from 50% to 100% between -1 and -0.5 (and 0.5 and 1, respectively).\n",
        "\n",
        "    `gravity` dictates the gravitational constant, this is bounded to be within 0 and -12.\n",
        "\n",
        "    If `enable_wind=True` is passed, there will be wind effects applied to the lander.\n",
        "    The wind is generated using the function `tanh(sin(2 k (t+C)) + sin(pi k (t+C)))`.\n",
        "    `k` is set to 0.01.\n",
        "    `C` is sampled randomly between -9999 and 9999.\n",
        "\n",
        "    `wind_power` dictates the maximum magnitude of linear wind applied to the craft. The recommended value for `wind_power` is between 0.0 and 20.0.\n",
        "    `turbulence_power` dictates the maximum magnitude of rotational wind applied to the craft. The recommended value for `turbulence_power` is between 0.0 and 2.0.\n",
        "\n",
        "    ### Version History\n",
        "    - v2: Count energy spent and in v0.24, added turbulance with wind power and turbulence_power parameters\n",
        "    - v1: Legs contact with ground added in state vector; contact with ground\n",
        "        give +10 reward points, and -10 if then lose contact; reward\n",
        "        renormalized to 200; harder initial random push.\n",
        "    - v0: Initial version\n",
        "\n",
        "    <!-- ### References -->\n",
        "\n",
        "    ### Credits\n",
        "    Created by Oleg Klimov\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": FPS,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: Optional[str] = None,\n",
        "        continuous: bool = False,\n",
        "        gravity: float = 0.0,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        turbulence_power: float = 1.5,\n",
        "        start_pos: tuple = (2, 7),\n",
        "        end_pos: tuple = (16, 7),\n",
        "    ):\n",
        "        EzPickle.__init__(\n",
        "            self,\n",
        "            render_mode,\n",
        "            continuous,\n",
        "            gravity,\n",
        "            enable_wind,\n",
        "            wind_power,\n",
        "            turbulence_power,\n",
        "        )\n",
        "\n",
        "        # assert (\n",
        "        #     -12.0 < gravity and gravity < 0.0\n",
        "        # ), f\"gravity (current value: {gravity}) must be between -12 and 0\"\n",
        "        self.gravity = gravity\n",
        "\n",
        "        if 0.0 > wind_power or wind_power > 20.0:\n",
        "            warnings.warn(\n",
        "                colorize(\n",
        "                    f\"WARN: wind_power value is recommended to be between 0.0 and 20.0, (current value: {wind_power})\",\n",
        "                    \"yellow\",\n",
        "                ),\n",
        "            )\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        if 0.0 > turbulence_power or turbulence_power > 2.0:\n",
        "            warnings.warn(\n",
        "                colorize(\n",
        "                    f\"WARN: turbulence_power value is recommended to be between 0.0 and 2.0, (current value: {turbulence_power})\",\n",
        "                    \"yellow\",\n",
        "                ),\n",
        "            )\n",
        "        self.turbulence_power = turbulence_power\n",
        "        self.start_pos = start_pos\n",
        "        self.end_pos = end_pos\n",
        "        self.enable_wind = enable_wind\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "        self.torque_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        self.screen: pygame.Surface = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.world = Box2D.b2World(gravity=[0.0, 0.0])\n",
        "        self.moon = None\n",
        "        self.lander: Optional[Box2D.b2Body] = None\n",
        "        self.particles = []\n",
        "\n",
        "        self.prev_reward = None\n",
        "\n",
        "        self.continuous = continuous\n",
        "\n",
        "        low = np.array(\n",
        "            [\n",
        "                -1.5,\n",
        "                -1.5,\n",
        "                -5.0,\n",
        "                -5.0,\n",
        "                -np.pi,\n",
        "                -5.0,\n",
        "                0.0,\n",
        "            ]\n",
        "        ).astype(np.float32)\n",
        "        high = np.array(\n",
        "            [\n",
        "                1.5,\n",
        "                1.5,\n",
        "                5.0,\n",
        "                5.0,\n",
        "                np.pi,\n",
        "                5.0,\n",
        "                np.pi,\n",
        "            ]\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        # useful range is -1 .. +1, but spikes can be higher\n",
        "        self.observation_space = spaces.Box(low, high)\n",
        "\n",
        "        if self.continuous:\n",
        "            # Action is two floats [main engine, left-right engines].\n",
        "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
        "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
        "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
        "        else:\n",
        "            # Nop, fire left engine, main engine, right engine\n",
        "            self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def _destroy(self):\n",
        "        if not self.moon:\n",
        "            return\n",
        "        self.world.contactListener = None\n",
        "        self._clean_particles(True)\n",
        "        self.world.DestroyBody(self.moon)\n",
        "        self.moon = None\n",
        "        self.world.DestroyBody(self.lander)\n",
        "        self.lander = None\n",
        "\n",
        "    def _is_within_bounds(self):\n",
        "        if 0 < self.lander.position[0] < 20 and 0 < self.lander.position[1] < 13:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _goal_pos_reached(self):\n",
        "        if (int(self.lander.position[0]), int(self.lander.position[1])) == self.end_pos:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _generate_random_start_and_end_pos(self, grid_size=(20, 13), boundary_distance=3):\n",
        "        # Randomly select start position near the boundaries\n",
        "        start_pos_x = np.random.choice([0, grid_size[0]-1])\n",
        "        start_pos_y = np.random.randint(boundary_distance, grid_size[1]-boundary_distance)\n",
        "        start_pos = (start_pos_x, start_pos_y)\n",
        "\n",
        "        # Determine initial angle of the lander based on the start position\n",
        "        if start_pos_x == 0:\n",
        "            init_angle = np.random.uniform(-np.pi, 0)\n",
        "        else:\n",
        "            init_angle = np.random.uniform(0, np.pi)\n",
        "\n",
        "        # Select end position on the opposite boundary\n",
        "        if start_pos_x == 0:\n",
        "            end_pos_x = grid_size[0] - 1\n",
        "        else:\n",
        "            end_pos_x = 0\n",
        "        end_pos_y = np.random.randint(boundary_distance, grid_size[1]-boundary_distance)\n",
        "        end_pos = (end_pos_x, end_pos_y)\n",
        "\n",
        "        self.start_pos = (int(start_pos[0]), int(start_pos[1]))\n",
        "        self.end_pos = (int(end_pos[0]), int(end_pos[1]))\n",
        "        self.init_angle = init_angle\n",
        "\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "        self._destroy()\n",
        "        self.prev_shaping = None\n",
        "\n",
        "        W = VIEWPORT_W / SCALE\n",
        "        H = VIEWPORT_H / SCALE\n",
        "\n",
        "        self.moon = self.world.CreateStaticBody(\n",
        "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
        "        )\n",
        "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
        "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
        "\n",
        "        self._generate_random_start_and_end_pos()\n",
        "\n",
        "        self.lander: Box2D.b2Body = self.world.CreateDynamicBody(\n",
        "            # position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
        "            position = self.start_pos,\n",
        "            angle=self.init_angle,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(\n",
        "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
        "                ),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x0000,  # do not collide with any object\n",
        "                restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "        self.lander.color1 = (128, 102, 230)\n",
        "        self.lander.color2 = (77, 77, 128)\n",
        "        # self.lander.ApplyForceToCenter(\n",
        "        #     (\n",
        "        #         self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "        #         self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "        #     ),\n",
        "        #     True,\n",
        "        # )\n",
        "\n",
        "        self.drawlist = [self.lander]\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self.step(np.array([0, 0]) if self.continuous else 0)[0], {}\n",
        "\n",
        "    def _create_particle(self, mass, x, y, ttl):\n",
        "        p = self.world.CreateDynamicBody(\n",
        "            position=(x, y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                density=mass,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0100,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.3,\n",
        "            ),\n",
        "        )\n",
        "        p.ttl = ttl\n",
        "        self.particles.append(p)\n",
        "        self._clean_particles(False)\n",
        "        return p\n",
        "\n",
        "    def _clean_particles(self, all):\n",
        "        while self.particles and (all or self.particles[0].ttl < 0):\n",
        "            self.world.DestroyBody(self.particles.pop(0))\n",
        "\n",
        "    def _angle_with_goal(self, pos):\n",
        "        agent_orientation = np.array([np.sin(self.lander.angle), np.cos(self.lander.angle)])\n",
        "        goal_vector = np.array([self.end_pos[0] - pos[0], self.end_pos[1] - pos[1]])\n",
        "        angle_with_goal = np.arccos(np.dot(agent_orientation, goal_vector) / (np.linalg.norm(agent_orientation) * np.linalg.norm(goal_vector)))\n",
        "        angle_with_goal -= np.pi\n",
        "        return -angle_with_goal\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.lander is not None\n",
        "\n",
        "        # Update wind\n",
        "        assert self.lander is not None, \"You forgot to call reset()\"\n",
        "        if self.enable_wind:\n",
        "            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            wind_mag = (\n",
        "                np.tanh(\n",
        "                    np.sin(0.02 * self.wind_idx)\n",
        "                    + (np.sin(np.pi * 0.01 * self.wind_idx))\n",
        "                )\n",
        "                * self.wind_power\n",
        "            )\n",
        "            self.wind_idx += 1\n",
        "            self.lander.ApplyForceToCenter(\n",
        "                (wind_mag, 0.0),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "            # the function used for torque is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            torque_mag = np.tanh(\n",
        "                np.sin(0.02 * self.torque_idx)\n",
        "                + (np.sin(np.pi * 0.01 * self.torque_idx))\n",
        "            ) * (self.turbulence_power)\n",
        "            self.torque_idx += 1\n",
        "            self.lander.ApplyTorque(\n",
        "                (torque_mag),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(\n",
        "                action\n",
        "            ), f\"{action!r} ({type(action)}) invalid \"\n",
        "\n",
        "        # Engines\n",
        "        tip = (np.sin(self.lander.angle), np.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            # 4 is move a bit downwards, +-2 for randomness\n",
        "            ox = tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        vel = self.lander.linearVelocity\n",
        "        state = [\n",
        "            (pos.x - self.end_pos[0]),\n",
        "            (pos.y - self.end_pos[1]),\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
        "            self.lander.angle,\n",
        "            20.0 * self.lander.angularVelocity / FPS,\n",
        "            self._angle_with_goal(pos),\n",
        "        ]\n",
        "        assert len(state) == 7\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
        "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "            - 100 * abs(state[4])\n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "#         reward -= (\n",
        "#             m_power * 0.30\n",
        "#         )  # less fuel spent is better, about -30 for heuristic landing\n",
        "#         reward -= s_power * 0.03\n",
        "\n",
        "        terminated = False\n",
        "        if not self._is_within_bounds():\n",
        "            terminated = True\n",
        "            reward = -100\n",
        "        if self._goal_pos_reached():\n",
        "            terminated = True\n",
        "            reward = +100\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return np.array(state, dtype=np.float32), reward, terminated, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            import pygame\n",
        "            from pygame import gfxdraw\n",
        "        except ImportError:\n",
        "            raise DependencyNotInstalled(\n",
        "                \"pygame is not installed, run `pip install gym[box2d]`\"\n",
        "            )\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((VIEWPORT_W, VIEWPORT_H))\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "        self.surf = pygame.Surface((VIEWPORT_W, VIEWPORT_H))\n",
        "\n",
        "        pygame.transform.scale(self.surf, (SCALE, SCALE))\n",
        "        pygame.draw.rect(self.surf, (255, 255, 255), self.surf.get_rect())\n",
        "\n",
        "        # Draw start and end markers\n",
        "        pygame.draw.circle(self.surf, (255, 0, 0), (self.start_pos[0]*SCALE, self.start_pos[1]*SCALE), 10)\n",
        "        pygame.draw.circle(self.surf, (0, 255, 0), (self.end_pos[0]*SCALE, self.end_pos[1]*SCALE), 10)\n",
        "\n",
        "        for obj in self.particles:\n",
        "            obj.ttl -= 0.15\n",
        "            obj.color1 = (\n",
        "                int(max(0.2, 0.15 + obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "            )\n",
        "            obj.color2 = (\n",
        "                int(max(0.2, 0.15 + obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "            )\n",
        "\n",
        "        self._clean_particles(False)\n",
        "\n",
        "        for obj in self.particles + self.drawlist:\n",
        "            for f in obj.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    pygame.draw.circle(\n",
        "                        self.surf,\n",
        "                        color=obj.color1,\n",
        "                        center=trans * f.shape.pos * SCALE,\n",
        "                        radius=f.shape.radius * SCALE,\n",
        "                    )\n",
        "                    pygame.draw.circle(\n",
        "                        self.surf,\n",
        "                        color=obj.color2,\n",
        "                        center=trans * f.shape.pos * SCALE,\n",
        "                        radius=f.shape.radius * SCALE,\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    path = [trans * v * SCALE for v in f.shape.vertices]\n",
        "                    pygame.draw.polygon(self.surf, color=obj.color1, points=path)\n",
        "                    gfxdraw.aapolygon(self.surf, path, obj.color1)\n",
        "                    pygame.draw.aalines(\n",
        "                        self.surf, color=obj.color2, points=path, closed=True\n",
        "                    )\n",
        "\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            assert self.screen is not None\n",
        "            self.screen.blit(self.surf, (0, 0))\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False\n",
        "\n",
        "\n",
        "def heuristic(env, s):\n",
        "    \"\"\"\n",
        "    The heuristic for\n",
        "    1. Testing\n",
        "    2. Demonstration rollout.\n",
        "\n",
        "    Args:\n",
        "        env: The environment\n",
        "        s (list): The state. Attributes:\n",
        "            s[0] is the horizontal coordinate\n",
        "            s[1] is the vertical coordinate\n",
        "            s[2] is the horizontal speed\n",
        "            s[3] is the vertical speed\n",
        "            s[4] is the angle\n",
        "            s[5] is the angular speed\n",
        "            s[6] 1 if first leg has contact, else 0\n",
        "            s[7] 1 if second leg has contact, else 0\n",
        "\n",
        "    Returns:\n",
        "         a: The heuristic to be fed into the step function defined above to determine the next step and reward.\n",
        "    \"\"\"\n",
        "\n",
        "    angle_targ = s[0] * 0.5 + s[2] * 1.0  # angle should point towards center\n",
        "    if angle_targ > 0.4:\n",
        "        angle_targ = 0.4  # more than 0.4 radians (22 degrees) is bad\n",
        "    if angle_targ < -0.4:\n",
        "        angle_targ = -0.4\n",
        "    hover_targ = 0.55 * np.abs(\n",
        "        s[0]\n",
        "    )  # target y should be proportional to horizontal offset\n",
        "\n",
        "    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5]) * 1.0\n",
        "    hover_todo = (hover_targ - s[1]) * 0.5 - (s[3]) * 0.5\n",
        "\n",
        "    if env.continuous:\n",
        "        a = np.array([hover_todo * 20 - 1, -angle_todo * 20])\n",
        "        a = np.clip(a, -1, +1)\n",
        "    else:\n",
        "        a = 0\n",
        "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05:\n",
        "            a = 2\n",
        "        elif angle_todo < -0.05:\n",
        "            a = 3\n",
        "        elif angle_todo > +0.05:\n",
        "            a = 1\n",
        "    return a\n",
        "\n",
        "\n",
        "def demo_heuristic_lander(env, seed=None, render=False):\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    s, info = env.reset(seed=seed)\n",
        "    while True:\n",
        "        # a = heuristic(env, s)\n",
        "        a = env.action_space.sample()\n",
        "        s, r, terminated, truncated, info = step_api_compatibility(env.step(a), True)\n",
        "        total_reward += r\n",
        "\n",
        "        if render:\n",
        "            still_open = env.render()\n",
        "            if still_open is False:\n",
        "                break\n",
        "\n",
        "        if steps % 20 == 0 or terminated or truncated:\n",
        "            print(\"observations:\", \" \".join([f\"{x:+0.2f}\" for x in s]))\n",
        "            print(f\"step {steps} reward {r:+0.2f} total_reward {total_reward:+0.2f}\")\n",
        "        steps += 1\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    if render:\n",
        "        env.close()\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "class LunarLanderContinuous:\n",
        "    def __init__(self):\n",
        "        raise error.Error(\n",
        "            \"Error initializing LunarLanderContinuous Environment.\\n\"\n",
        "            \"Currently, we do not support initializing this mode of environment by calling the class directly.\\n\"\n",
        "            \"To use this environment, instead create it by specifying the continuous keyword in gym.make, i.e.\\n\"\n",
        "            'gym.make(\"LunarLander-v2\", continuous=True)'\n",
        "        )\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     demo_heuristic_lander(LunarLander(render_mode = 'human', enable_wind=False), render=True)\n",
        "\n",
        "def play_DQN_episode(env, agent):\n",
        "    score = 0\n",
        "    state, _ = env.reset(seed=42)\n",
        "\n",
        "    while True:\n",
        "        # eps=0 for predictions\n",
        "        action = agent.act(state, 0)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        env.render()\n",
        "\n",
        "        # End the episode if done\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return score\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-11T18:20:45.94216Z",
          "iopub.execute_input": "2024-05-11T18:20:45.9425Z",
          "iopub.status.idle": "2024-05-11T18:20:46.050188Z",
          "shell.execute_reply.started": "2024-05-11T18:20:45.94247Z",
          "shell.execute_reply": "2024-05-11T18:20:46.049505Z"
        },
        "trusted": true,
        "id": "JweWmEkm71yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/tQ3zeQA.gif)\n",
        "\n",
        "## General Information\n",
        "This information is from the official Gym documentation.\n",
        "\n",
        "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
        "\n",
        "| Feature Category  | Details                                |\n",
        "|-------------------|----------------------------------------|\n",
        "| Action Space      | Discrete(4)                            |\n",
        "| Observation Shape | (8,)                                   |\n",
        "| Observation High  | [1.5 1.5 5. 5. 3.14 5. 1. 1. ]         |\n",
        "| Observation Low   | [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] |\n",
        "| Import            | `gym.make(\"LunarLander-v2\")`           |\n",
        "\n",
        "## Description of Environment\n",
        "\n",
        "This environment is a classic rocket trajectory optimization problem. According to Pontryaginâ€™s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates `(0,0)`. The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent could learn to fly and then land on its first attempt.\n",
        "\n",
        "## Action Space\n",
        "There are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
        "\n",
        "| Action  | Result                          |\n",
        "|---------|---------------------------------|\n",
        "| 0       | Do nothing                      |\n",
        "| 1       | Fire left orientation engine    |\n",
        "| 2       | Fire main engine                |\n",
        "| 3       | Fire right orientation engine   |\n",
        "\n",
        "## Observation Space\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear velocities in `x` & `y`, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "\n",
        "| Observation  | Value                                   |\n",
        "|--------------|-----------------------------------------|\n",
        "| 0            | `x` coordinate (float)                  |\n",
        "| 1            | `y` coordinate (float)                  |\n",
        "| 2            | `x` linear velocity (float)             |\n",
        "| 3            | `y` linear velocity (float)             |\n",
        "| 4            | Angle in radians from -Ï€ to +Ï€ (float)  |\n",
        "| 5            | Angular velocity (float)                |\n",
        "| 6            | Left leg contact (bool)                 |\n",
        "| 7            | Right leg contact (bool)                |\n",
        "\n",
        "## Rewards\n",
        "Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points.\n",
        "\n",
        "## Starting State\n",
        "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
        "\n",
        "## Episode Termination\n",
        "The episode finishes if:\n",
        "\n",
        "1. The lander crashes (the lander body gets in contact with the moon);\n",
        "\n",
        "2. The lander gets outside of the viewport (`x` coordinate is greater than 1);\n",
        "\n",
        "3. The lander is not awake. From the Box2D docs, a body which is not awake is a body which doesnâ€™t move and doesnâ€™t collide with any other body:\n",
        "\n",
        "---\n",
        "\n",
        "## The Safe Agent\n",
        "We're going to implement a simple agent 'The Safe Agent' who will thrust upward if and only if the lander's `y` position is less than 0.5.\n",
        "\n",
        "In theory this agent shouldn't hit the ground as we have unlimited fuel, but let's see."
      ],
      "metadata": {
        "id": "_EovjIF171yI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/qFNn9ai.gif)\n",
        "\n",
        "#### Observations:\n",
        "- The safe agent may not have hit the ground, but it didn't take long to fly off screen, due to its inability to use the side engines.\n",
        "\n",
        "---\n",
        "\n",
        "## The Stable Agent\n",
        "Let's try to define and agent that can remain stable in the air.\n",
        "\n",
        "It will operate via the following rules:\n",
        "\n",
        "1. If below height of 1: action = 2 (main engine)\n",
        "2. If angle is above Ï€/50: action = 1 (fire right engine)\n",
        "3. If angle is above Ï€/50: action = 1 (fire left engine)\n",
        "4. If x distance is above 0.4: action = 3 (fire left engine)\n",
        "5. If x distance is below -0.4: action = 1 (fire left engine)\n",
        "6. If below height of 1.5: action = 2 (main engine)\n",
        "6. Else: action = 0 (do nothing)\n",
        "\n",
        "The idea is the lander will always use its main engine if it falls below a certain height, next it will prioritize stabilizing the angle of the lander, then the distance, then keeping it above another height.\n",
        "\n",
        "Let's see how this approach does:"
      ],
      "metadata": {
        "id": "-XK0AOZL71yK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/Bdq1Hdl.gif)\n",
        "\n",
        "#### Observations:\n",
        "- Crafting a straightforward set of rules to guide the lunar lander is more challenging than anticipated.\n",
        "- Our initial efforts achieved some stability, but eventually, the lander lost control.\n",
        "\n",
        "---\n",
        "\n",
        "# Deep Reinforcement Learning\n",
        "To address this challenge, we'll use deep reinforcement learning techniques to train an agent to land the spacecraft.\n",
        "\n",
        "Simpler tabular methods are limited to discrete observation spaces, meaning there are a finite number of possible states. In `LunarLander-v2` however, we're dealing with a continuous range of states across 8 different parameters, meaning there are a near-infinite number of possible states. We could try to bin similar values into groups, but due to the sensitive controls of the game, even slight errors can lead to significant missteps.\n",
        "\n",
        "To get around this, we'll use a `neural network Q-function approximator`. This lets us predict the best actions to take for a given state, even when dealing with a vast number of potential states. It's a much better match for our complex landing challenge.\n",
        "\n",
        "## The DQN Algorithm:\n",
        "\n",
        "This breakthrough algorithm was used by Mihn et al in 2015 to achieve human-level performance on several Atari 2600 games.\n",
        "\n",
        "The original paper published in Nature can be viewed here:\n",
        "\n",
        "https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning\n",
        "\n",
        "The algorithm:\n",
        "\n",
        "1. **Initialization**: Begin by initializing the parameters for two neural networks, $Q(s,a)$ (referred to as the online network) and $\\hat{Q}(s,a)$ (known as the target network), with random weights. Both networks serve the function of mapping a state-action pair to a Q-value, which is an estimate of the expected return from that pair. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer to store past transition experiences.\n",
        "2. **Action Selection**: Utilize an epsilon-greedy strategy for action selection. With a probability of $\\epsilon$, select a random action $a$, but in all other instances, choose the action $a$ that maximizes the Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "3. **Experience Collection**: Execute the chosen action $a$ within the environment emulator and observe the resulting immediate reward $r$ and the next state $s'$.\n",
        "4. **Experience Storage**: Store the transition $(s,a,r,s')$ in the replay buffer for future reference.\n",
        "5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer for training the online network.\n",
        "6. **Target Computation**: For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended at this step, $y$ is simply the reward $r$. Otherwise, $y$ is the sum of the reward and the discounted estimated optimal future Q-value, i.e.,  $y = r + \\gamma \\max_{a' \\in A} \\hat{Q}(s', a')$\n",
        "7. **Loss Calculation**: Compute the loss, which is the squared difference between the Q-value predicted by the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$\n",
        "8. **Online Network Update**: Update the parameters of the online network $Q(s,a)$ using Stochastic Gradient Descent (SGD) to minimize the loss.\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network to the target network $\\hat{Q}(s,a)$.\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "### Defining the Deep Q-Network\n",
        "Our network will be a simple feedforward neural network that takes the state as input and produces Q-values for each action as output. For `LunarLander-v2` the state is an 8-dimensional vector and there are 4 possible actions.\n"
      ],
      "metadata": {
        "id": "BhauwO-l71yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class DQN(torch.nn.Module):\n",
        "    '''\n",
        "    This class defines a deep Q-network (DQN), a type of artificial neural network used in reinforcement learning.\n",
        "    The DQN is used to estimate the Q-values, which represent the expected return for each action in each state.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state_size: int, default=8\n",
        "        The size of the state space.\n",
        "    action_size: int, default=4\n",
        "        The size of the action space.\n",
        "    hidden_size: int, default=64\n",
        "        The size of the hidden layers in the network.\n",
        "    '''\n",
        "    def __init__(self, state_size=7, action_size=4, hidden_size=64):\n",
        "        '''\n",
        "        Initialize a network with the following architecture:\n",
        "            Input layer (state_size, hidden_size)\n",
        "            Hidden layer 1 (hidden_size, hidden_size)\n",
        "            Output layer (hidden_size, action_size)\n",
        "        '''\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        '''\n",
        "        Define the forward pass of the DQN. This function is called when the network is called to estimate Q-values.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: torch.Tensor\n",
        "            The state for which to estimate the Q-values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The estimated Q-values for each action in the input state.\n",
        "        '''\n",
        "        x = torch.relu(self.layer1(state))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-11T18:20:46.051206Z",
          "iopub.execute_input": "2024-05-11T18:20:46.051485Z",
          "iopub.status.idle": "2024-05-11T18:20:48.991418Z",
          "shell.execute_reply.started": "2024-05-11T18:20:46.051462Z",
          "shell.execute_reply": "2024-05-11T18:20:48.990651Z"
        },
        "trusted": true,
        "id": "kXp1K9nB71yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Replay Buffer\n",
        "In the context of RL, we employ a structure known as the replay buffer, which utilizes a deque. The replay buffer stores and samples experiences, which helps us overcome the problem of *step correlation*.\n",
        "\n",
        "A *deque* (double-ended queue) is a data structure that enables the addition or removal of elements from both its ends, hence the name. It is particularly useful when there is a need for fast append and pop operations from either end of the container, which it provides at O(1) time complexity. In contrast, a list offers these operations at O(n) time complexity, making the deque a preferred choice in cases that necessitate more efficient operations.\n",
        "\n",
        "Moreover, a deque allows setting a maximum size. Once this maximum size is exceeded during an insertion (push) operation at the front, the deque automatically ejects the item at the rear, thereby maintaining its maximum length.\n",
        "\n",
        "In the replay buffer, the `push` method is utilized to add an experience. If adding this experience exceeds the maximum buffer size, the oldest (rear-most) experience is automatically removed. This approach ensures that the replay buffer always contains the most recent experiences up to its capacity.\n",
        "\n",
        "The `sample` method, on the other hand, is used to retrieve a random batch of experiences from the replay buffer. This randomness is critical in breaking correlations within the sequence of experiences, which leads to more robust learning.\n",
        "\n",
        "This combination of recency and randomness allows us to learn on new training data, without training samples being highly correlated."
      ],
      "metadata": {
        "id": "PalEcq4J71yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    '''\n",
        "    This class represents a replay buffer, a type of data structure commonly used in reinforcement learning algorithms.\n",
        "    The buffer stores past experiences in the environment, allowing the agent to sample and learn from them at later times.\n",
        "    This helps to break the correlation of sequential observations and stabilize the learning process.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    buffer_size: int, default=10000\n",
        "        The maximum number of experiences that can be stored in the buffer.\n",
        "    '''\n",
        "    def __init__(self, buffer_size=10000):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Add a new experience to the buffer. Each experience is a tuple containing a state, action, reward,\n",
        "        the resulting next state, and a done flag indicating whether the episode has ended.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The state of the environment before taking the action.\n",
        "        action: int\n",
        "            The action taken by the agent.\n",
        "        reward: float\n",
        "            The reward received after taking the action.\n",
        "        next_state: array-like\n",
        "            The state of the environment after taking the action.\n",
        "        done: bool\n",
        "            A flag indicating whether the episode has ended after taking the action.\n",
        "        '''\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Randomly sample a batch of experiences from the buffer. The batch size must be smaller or equal to the current number of experiences in the buffer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            The number of experiences to sample from the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of numpy.ndarray\n",
        "            A tuple containing arrays of states, actions, rewards, next states, and done flags.\n",
        "        '''\n",
        "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Get the current number of experiences in the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of experiences in the buffer.\n",
        "        '''\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-11T18:20:48.993414Z",
          "iopub.execute_input": "2024-05-11T18:20:48.993918Z",
          "iopub.status.idle": "2024-05-11T18:20:49.002963Z",
          "shell.execute_reply.started": "2024-05-11T18:20:48.993892Z",
          "shell.execute_reply": "2024-05-11T18:20:49.00201Z"
        },
        "trusted": true,
        "id": "V9UFBe4m71yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the DQN Agent\n",
        "The DQN agent handles the interaction with the environment, selecting actions, collecting experiences, storing them in the replay buffer, and using these experiences to train the network. Let's walk through each part of this process:\n",
        "\n",
        "#### Initialisation\n",
        "The `__init__` function sets up the agent:\n",
        "\n",
        "- `self.device`: We start by checking whether a GPU is available, and, if so, we use it, otherwise, we fall back to CPU.\n",
        "- `self.gamma`: This is the discount factor for future rewards, used in the Q-value update equation.\n",
        "- `self.batch_size`: This is the number of experiences we'll sample from the memory when updating the model.\n",
        "- `self.q_network` and `self.target_network`: These are two instances of the Q-Network. The first is the network we're actively training, and the second is a copy that gets updated less frequently. This helps to stabilize learning.\n",
        "- `self.optimizer`: This is the optimization algorithm used to update the Q-Network's parameters.\n",
        "- `self.memory`: This is a replay buffer that stores experiences. It's an instance of the `ReplayBuffer` class.\n",
        "\n",
        "#### Step Function\n",
        "The `step` function is called after each timestep in the environment:\n",
        "\n",
        "- The function starts by storing the new experience in the replay buffer.\n",
        "- If enough experiences have been stored, it calls `self.update_model()`, which triggers a learning update.\n",
        "\n",
        "#### Action Selection\n",
        "The act function is how the agent selects an action:\n",
        "\n",
        "- If a randomly drawn number is greater than $\\epsilon$, it selects the action with the highest predicted Q-value. This is known as exploitation: the agent uses what it has learned to select the best action.\n",
        "- If the random number is less than $\\epsilon$, it selects an action randomly. This is known as exploration: the agent explores the environment to learn more about it.\n",
        "\n",
        "#### Model Update\n",
        "The `update_model` function is where the learning happens:\n",
        "\n",
        "- It starts by sampling a batch of experiences from the replay buffer.\n",
        "- It then calculates the current Q-values for the sampled states and actions, and the expected - Q-values based on the rewards and next states.\n",
        "- It calculates the loss, which is the mean squared difference between the current and expected Q-values.\n",
        "- It then backpropagates this loss through the Q-Network and updates the weights using the optimizer.\n",
        "\n",
        "#### Target Network Update\n",
        "Finally, the `update_target_network` function copies the weights from the Q-Network to the Target Network. This is done periodically (not every step), to stabilize the learning process. Without this, the Q-Network would be trying to follow a moving target, since it's learning from estimates produced by itself."
      ],
      "metadata": {
        "id": "-Ei_LTTE71yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    '''\n",
        "    This class represents a Deep Q-Learning agent that uses a Deep Q-Network (DQN) and a replay memory to interact\n",
        "    with its environment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state_size: int, default=8\n",
        "        The size of the state space.\n",
        "    action_size: int, default=4\n",
        "        The size of the action space.\n",
        "    hidden_size: int, default=64\n",
        "        The size of the hidden layers in the network.\n",
        "    learning_rate: float, default=1e-3\n",
        "        The learning rate for the optimizer.\n",
        "    gamma: float, default=0.99\n",
        "        The discount factor for future rewards.\n",
        "    buffer_size: int, default=10000\n",
        "        The maximum size of the replay memory.\n",
        "    batch_size: int, default=64\n",
        "        The batch size for learning from the replay memory.\n",
        "    '''\n",
        "    def __init__(self, state_size=7, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Discount factor for future rewards\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Batch size for sampling from the replay memory\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Number of possible actions\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n",
        "        # Move the networks to the selected device\n",
        "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "\n",
        "        # Set weights of target network to be the same as those of the q network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Set target network to evaluation mode\n",
        "        self.target_network.eval()\n",
        "\n",
        "        # Initialize the optimizer for updating the Q-Network's parameters\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Initialize the replay memory\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Perform a step in the environment, store the experience in the replay memory and potentially update the Q-network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The current state of the environment.\n",
        "        action: int\n",
        "            The action taken by the agent.\n",
        "        reward: float\n",
        "            The reward received after taking the action.\n",
        "        next_state: array-like\n",
        "            The state of the environment after taking the action.\n",
        "        done: bool\n",
        "            A flag indicating whether the episode has ended after taking the action.\n",
        "        '''\n",
        "        # Store the experience in memory\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "        # If there are enough experiences in memory, perform a learning step\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        '''\n",
        "        Choose an action based on the current state and the epsilon-greedy policy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The current state of the environment.\n",
        "        eps: float, default=0.\n",
        "            The epsilon for the epsilon-greedy policy. With probability eps, a random action is chosen.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The chosen action.\n",
        "        '''\n",
        "        # If a randomly chosen value is greater than eps\n",
        "        if random.random() > eps:\n",
        "            # Convert state to a PyTorch tensor and set network to evaluation mode\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "\n",
        "            # With no gradient updates, get the action values from the DQN\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "\n",
        "            # Revert to training mode and return action\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            # Return a random action for random value > eps\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get maximum Q-value for the next states from target network\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        '''\n",
        "        Update the weights of the target network to match those of the Q-network.\n",
        "        '''\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-11T18:20:49.003958Z",
          "iopub.execute_input": "2024-05-11T18:20:49.004274Z",
          "iopub.status.idle": "2024-05-11T18:20:49.026596Z",
          "shell.execute_reply.started": "2024-05-11T18:20:49.00425Z",
          "shell.execute_reply": "2024-05-11T18:20:49.025767Z"
        },
        "trusted": true,
        "id": "H9wEfeJV71yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Agent\n",
        "\n",
        "Training the agent involves having the agent interact with the `LunarLander-v2` environment over a sequence of steps. Over each step, the agent receives a state from the environment, selects an action, receives a reward and the next state, and then updates its understanding of the environment (the Q-table in the case of Q-Learning).\n",
        "\n",
        "The `train` function orchestrates this process over a defined number of episodes, using the methods defined in the DQNAgent class. Here's how it works:\n",
        "\n",
        "#### Initial Setup\n",
        "- `scores`: This list stores the total reward obtained in each episode.\n",
        "- `scores_window`: This is a double-ended queue with a maximum length of 100. It holds the scores of the most recent 100 episodes and is used to monitor the agent's performance.\n",
        "-`eps`: This is the epsilon for epsilon-greedy action selection. It starts from `eps_start` and decays after each episode until it reaches `eps_end`.\n",
        "\n",
        "#### Episode Loop\n",
        "The training process runs over a fixed number of episodes. In each episode:\n",
        "\n",
        "- The environment is reset to its initial state.\n",
        "- he agent then interacts with the environment until the episode is done (when a terminal state is reached).\n",
        "\n",
        "#### Step Loop\n",
        "In each step of an episode:\n",
        "\n",
        "- The agent selects an action using the current policy (the act method in `DQNAgent`).\n",
        "The selected action is applied to the environment using the step method, which returns the next state, the reward, and a boolean indicating whether the episode is done.\n",
        "- The agent's step method is called to update the agent's knowledge. This involves adding the experience to the replay buffer and, if enough experiences have been collected, triggering a learning update.\n",
        "- The state is updated to the next state, and the reward is added to the score.\n",
        "\n",
        "After each episode:\n",
        "\n",
        "- The score for the episode is added to `scores` and `scores_window`.\n",
        "- Epsilon is decayed according to `eps_decay`.\n",
        "- If the episode is a multiple of `target_update`, the target network is updated with the latest weights from the Q-Network.\n",
        "- Finally, every 100 episodes, the average score over the last 100 episodes is printed.\n",
        "\n",
        "The function returns the list of scores for all episodes.\n",
        "\n",
        "This training process, which combines experiences from the replay buffer and separate target and Q networks, helps to stabilize the learning and leads to a more robust policy."
      ],
      "metadata": {
        "id": "lbrthx4A71yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n",
        "    '''\n",
        "    Train a DQN agent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    agent: DQNAgent\n",
        "        The agent to be trained.\n",
        "    env: gym.Env\n",
        "        The environment in which the agent is trained.\n",
        "    n_episodes: int, default=2000\n",
        "        The number of episodes for which to train the agent.\n",
        "    eps_start: float, default=1.0\n",
        "        The starting epsilon for epsilon-greedy action selection.\n",
        "    eps_end: float, default=0.01\n",
        "        The minimum value that epsilon can reach.\n",
        "    eps_decay: float, default=0.995\n",
        "        The decay rate for epsilon after each episode.\n",
        "    target_update: int, default=10\n",
        "        The frequency (number of episodes) with which the target network should be updated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of float\n",
        "        The total reward obtained in each episode.\n",
        "    '''\n",
        "\n",
        "    # Initialize the scores list and scores window\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    # Loop over episodes\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "\n",
        "        # Reset environment and score at the start of each episode\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        # Loop over steps\n",
        "        while True:\n",
        "\n",
        "            # Select an action using current agent policy then apply in environment\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update the agent, state and score\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # End the episode if done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # At the end of episode append and save scores\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Decrease epsilon\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        # Print some info\n",
        "        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
        "\n",
        "        # Update target network every target_update episodes\n",
        "        if i_episode % target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        # Print average score every 100 episodes\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        # This environment is considered to be solved for a mean score of 200 or greater, so stop training.\n",
        "#         if i_episode % 100 == 0 and np.mean(scores_window) >= 200:\n",
        "#             break\n",
        "\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Make an environment\n",
        "env = LunarLander()\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DQN agent\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-11T18:20:49.027716Z",
          "iopub.execute_input": "2024-05-11T18:20:49.027989Z",
          "iopub.status.idle": "2024-05-11T18:31:10.024565Z",
          "shell.execute_reply.started": "2024-05-11T18:20:49.027965Z",
          "shell.execute_reply": "2024-05-11T18:31:10.023683Z"
        },
        "trusted": true,
        "id": "K5mMgDRo71yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-11T18:31:33.582738Z",
          "iopub.execute_input": "2024-05-11T18:31:33.583136Z",
          "iopub.status.idle": "2024-05-11T18:31:33.884174Z",
          "shell.execute_reply.started": "2024-05-11T18:31:33.583107Z",
          "shell.execute_reply": "2024-05-11T18:31:33.883148Z"
        },
        "trusted": true,
        "id": "JGyBEM_l71yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "import pickle\n",
        "with open('agent.pkl', 'wb') as f:\n",
        "    pickle.dump(agent, f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-11T18:31:10.337545Z",
          "iopub.execute_input": "2024-05-11T18:31:10.337879Z",
          "iopub.status.idle": "2024-05-11T18:31:10.477528Z",
          "shell.execute_reply.started": "2024-05-11T18:31:10.337852Z",
          "shell.execute_reply": "2024-05-11T18:31:10.476507Z"
        },
        "trusted": true,
        "id": "IHhuklkd71yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "- Our DQN agent is able to solve the game typically after playing around 1200 episodes.\n",
        "- Let's watch a video of this agent's performance:"
      ],
      "metadata": {
        "id": "4sMHEScx71yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = LunarLander(render_mode='human')\n",
        "\n",
        "def play_DQN_episode(env, agent):\n",
        "    score = 0\n",
        "    state, _ = env.reset(seed=42)\n",
        "\n",
        "    while True:\n",
        "        # eps=0 for predictions\n",
        "        action = agent.act(state, 0)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # End the episode if done\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return score\n",
        "\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "2qYjbKmr71yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/NAg48Qk.gif)\n",
        "\n",
        "## Double DQN (DDQN)\n",
        "The Double Deep Q-Network (DDQN) algorithm is a modification of the standard Deep Q-Network (DQN) algorithm, which reduces the overestimation bias in the Q-values, thereby improving the stability of the learning process. You can read the original publication by Hasselt et al from late 2015 here:\n",
        "\n",
        "https://arxiv.org/abs/1509.06461\n",
        "\n",
        "### The DDQN Algorithm\n",
        "\n",
        "1. **Initialization**: Similar to DQN, initialize the parameters of two neural networks, $Q(s,a)$ (online network) and $\\hat{Q}(s,a)$ (target network), with random weights. Both networks estimate Q-values from state-action pairs. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer.\n",
        "\n",
        "2. **Action Selection**: Use an epsilon-greedy strategy, just like in DQN. With a probability of $\\epsilon$, select a random action $a$, otherwise, select the action $a$ that yields the highest Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "\n",
        "3. **Experience Collection**: Carry out the selected action $a$ in the environment to get the immediate reward $r$ and the next state $s'$.\n",
        "\n",
        "4. **Experience Storage**: Store the transition tuple $(s,a,r,s')$ in the replay buffer.\n",
        "\n",
        "5. **Sampling:** Randomly sample a mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. **Target Computation**: Here comes the primary difference from DQN. For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended, $y = r$. Otherwise, unlike DQN that uses the max operator to select the action from the target network, DDQN uses the online network to select the best action, and uses its Q-value estimate from the target network, i.e., $y = r + \\gamma \\hat{Q}(s', argmax_{a' \\in A} Q(s', a'))$. This double estimator approach helps to reduce overoptimistic value estimates.\n",
        "\n",
        "7. **Loss Calculation**: Compute the loss as the squared difference between the predicted Q-value from the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$.\n",
        "\n",
        "8. **Online Network Update**: Perform Stochastic Gradient Descent (SGD) on the online network to minimize the loss.\n",
        "\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network.\n",
        "\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "In summary, the key difference in DDQN lies in the way the target Q-value is calculated for non-terminal states during the update. DDQN chooses the action using the online network and estimates the Q-value for this action using the target network. This modification helps mitigate the issue of overestimation present in standard DQN.\n"
      ],
      "metadata": {
        "id": "TPfBFgCs71yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQNAgent:\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get the action values from the online network\n",
        "        next_action_values = self.q_network(next_states).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "        # Get the Q-values from the target network for the actions chosen by the Q-network\n",
        "        next_q_values = self.target_network(next_states).gather(1, next_action_values).detach().squeeze(-1)\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DQN agent\n",
        "agent = DDQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n",
        "\n",
        "# Play a demonstration episode\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "ucPLM8E371yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/rrfB9Vl.gif)\n",
        "\n",
        "## Dueling Deep Q-Networks (Dueling DQN)\n",
        "The Dueling Deep Q-Network (Dueling DQN) algorithm is an extension of the standard Deep Q-Network (DQN) algorithm, which aims to improve the estimation of the state-value function and thus enhance the quality of the policy. The dueling architecture was proposed by Wang et al in 2015, and you can find their original paper here:\n",
        "\n",
        "https://arxiv.org/abs/1511.06581\n",
        "\n",
        "### The Dueling DQN Algorithm\n",
        "1. **Initializatin**: In Dueling DQN, initialize the parameters of two neural networks, $Q(s,a)$ (online network) and $\\hat{Q}(s,a)$ (target network), with random weights. Unlike the traditional DQN, each network in Dueling DQN splits into two separate streams at some point - one for estimating the state-value function $V(s)$ and the other for estimating the advantage function $A(s,a)$. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer.\n",
        "\n",
        "2. **Action Selection**: The action selection process is the same as DQN. Use an epsilon-greedy strategy. With a probability of $\\epsilon$, select a random action $a$, otherwise, select the action $a$ that yields the highest Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "\n",
        "3. **Experience Collection**: Carry out the selected action $a$ in the environment to obtain the immediate reward $r$ and the next state $s'$.\n",
        "\n",
        "4. **Experience Storage**: Store the transition tuple $(s,a,r,s')$ in the replay buffer.\n",
        "\n",
        "5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. **Target Computation**: For each transition in the sampled mini-batch, compute the target value $y$. If the episode has ended, $y = r$. Otherwise, compute $y$ as $y = r + \\gamma \\hat{Q}(s', argmax_{a' \\in A} Q(s', a'))$.\n",
        "\n",
        "7. **Loss Calculation**: Compute the loss as the squared difference between the predicted Q-value from the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$.\n",
        "\n",
        "8. **Online Network Update**: Use Stochastic Gradient Descent (SGD) or another optimization algorithm to update the online network and minimize the loss.\n",
        "\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network.\n",
        "\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "Dueling DQN indeed introduces a novel network architecture for approximating the Q-value function. It separates the Q-value into two parts: the state-value function $V(s)$, which estimates the value of a state regardless of the actions, and the advantage function $A(s,a)$, which measures the relative advantage of taking an action in a state compared to the other actions.\n",
        "\n",
        "At first glance, it might seem logical to compute the Q-value simply by adding the state-value and the advantage: $Q(s,a) = V(s) + A(s,a)$. However, this equation presents an issue: it's underdetermined. There are infinite possible combinations of $V(s)$ and $A(s,a)$ that satisfy this equation for a given $Q(s,a)$. For instance, if the actual value of $Q(s,a)$ is 10, we would have the equation $10 = V(s) + A(s,a)$, for which there are infinite solutions.\n",
        "\n",
        "The authors of the Dueling DQN paper propose a clever way to overcome this issue: they force the advantage function to have zero advantage at the chosen action. This means that the highest advantage, $A(s,a)$, is 0, and other advantages are negative or zero, thus providing a unique solution. To implement this, they modify the equation as follows:\n",
        "\n",
        "$$ Q(s,a) = V(s)+(A(s,a) âˆ’ \\max_{a'}A(s, a') $$\n",
        "\n",
        "This equation means that the Q-value is computed as the state-value $V(s)$ plus the difference between the advantage of the action $a$ and the maximum advantage over all possible actions in state $s$. In other words, the Q-value is now the value of the state plus the relative advantage of taking the action $a$ over the other actions. This mechanism provides a clear way to train the network and allows Dueling DQN to learn efficiently about state values and action advantages.\n",
        "\n",
        "To implement this, we can use the original DQN algorithm and our original DQNAgent class, we just need to change the DQN it uses, in total just 2 lines of code changes in the agent class."
      ],
      "metadata": {
        "id": "WE4_Jihy71yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DuelingDQN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
        "\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        # Common layers\n",
        "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Advantage layer\n",
        "        self.advantage = torch.nn.Linear(hidden_size, action_size)\n",
        "\n",
        "        # Value layer\n",
        "        self.value = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        # Common part of the network\n",
        "        x = torch.relu(self.layer1(state))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "\n",
        "        # Streams split here\n",
        "        advantage = self.advantage(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        # Recombine advantage and value for Q\n",
        "        return value + (advantage - advantage.max(dim=1, keepdim=True)[0])\n",
        "\n",
        "\n",
        "class DuelingDQNAgent:\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Use the dueling DQN networks instead\n",
        "        self.q_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DuelingDQN agent\n",
        "agent = DuelingDQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n",
        "\n",
        "# Play a demonstration episode\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "-m_RQevF71yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/QNCmfd3.gif)\n",
        "\n",
        "## Double Dueling Deep Q-Networks (Dueling DQN)\n",
        "\n",
        "We can also use the DDQN training trick to prevent the overestimation of Q-values from Dueling DQN. We can call this algorithm Dueling Double Deep Q-Network, or D3QN.\n",
        "\n",
        "To use this, we just need to change the code in our DuelingDQN agent's `update_model` method so it uses the DDQN trick to prevent Q-value overestimation:"
      ],
      "metadata": {
        "id": "FvMPKtdm71yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class D3QNAgent:\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "        self.q_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get the action values from the online network\n",
        "        next_action_values = self.q_network(next_states).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "        # Get the Q-values from the target network for the actions chosen by the Q-network\n",
        "        next_q_values = self.target_network(next_states).gather(1, next_action_values).detach().squeeze(-1)\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        '''\n",
        "        Update the weights of the target network to match those of the Q-network.\n",
        "        '''\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a D3QN agent\n",
        "agent = D3QNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n",
        "\n",
        "# Play a demonstration episode\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "0fsgmr3B71yW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}